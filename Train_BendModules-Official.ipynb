{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from bendable_gan import BendedGenerator\n",
    "from bending_modules import BendingConvModule, BendingConvModule_XY, \\\n",
    "    BendingCPPN, BendingDiffSort, BendingDiffSort_XY, ConcatenatedModules\n",
    "from losses import compute_diversity_loss\n",
    "from utils import generate_image, generate_image_from_seed, image_grid\n",
    "from clip import TextPrompt, NCELoss\n",
    "import gc\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [i for i in range(16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanillagen = BendedGenerator.from_pretrained(\"ceyda/butterfly_cropped_uniq1K_512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image_from_seed(vanillagen, seed=seeds[i]) for i in range(16)]\n",
    "for i, img in enumerate(exampleimgs):\n",
    "    img.save(f\"butterfly_vanilla_{seeds[i]}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Loss (+ diversity losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new bending module to optimize\n",
    "# with CLIP loss\n",
    "\n",
    "numchans = [1024, 1024, 512, 256, 128, 64, 6]\n",
    "\n",
    "bending_idx = 5\n",
    "\n",
    "bendingmod_clip = BendingConvModule(numchans[bending_idx],\n",
    "                                    act_fn='sin')\n",
    "\n",
    "bend_generator_clip = BendedGenerator.from_pretrained(\"ceyda/butterfly_cropped_uniq1K_512\",\n",
    "                                                 bending_module=bendingmod_clip,\n",
    "                                                 bending_idx=bending_idx,\n",
    "                                                 train_bending=True)\n",
    "bend_generator_clip = bend_generator_clip.to(device)\n",
    "\n",
    "\n",
    "tgt_text = 'Low-poly rendering of Benjamin Franklin going to Venice'\n",
    "tgt_text = 'Dinosaur tiffany lamp'\n",
    "tgt_text = 'A tree painted by Cezanne'\n",
    "text_prompt = TextPrompt(tgt_text, device=device)\n",
    "nce_loss = NCELoss(tgt_text, device=device, temperature=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "import random\n",
    "random.seed(23456)\n",
    "np.random.seed(54321)\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "n_iter = 1000\n",
    "\n",
    "div_loss = False\n",
    "div_weight = 6.\n",
    "div_loss_clip = True\n",
    "div_clip_weight = 6.\n",
    "\n",
    "opt = Adam(bendingmod_clip.parameters(), 1e-3)\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "for i in tqdm(range(n_iter)):\n",
    "    \n",
    "    noise_input = torch.randn(batch_size, \n",
    "                    bend_generator_clip.latent_dim, \n",
    "                    device=device)\n",
    "    \n",
    "    out, b_in, _ = bend_generator_clip(noise_input, return_inout=True)\n",
    "    out = out.clamp_(0., 1.)\n",
    "        \n",
    "    if div_loss_clip:\n",
    "        loss, clip_div = text_prompt(out, diversity=True)\n",
    "        loss += div_clip_weight * clip_div\n",
    "    else:\n",
    "        loss = nce_loss(out) #text_prompt(out)\n",
    "    if div_loss:\n",
    "        loss += div_weight * compute_diversity_loss(out, b_in)\n",
    "    \n",
    "    loss_log.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    \n",
    "plt.plot(range(n_iter), loss_log)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image_from_seed(bend_generator_clip, seed=seeds[i]) for i in range(16)]\n",
    "\n",
    "ts = tgt_text.replace(\" \", \"_\")\n",
    "for i, img in enumerate(exampleimgs):\n",
    "    img.save(f\"butterfly_clip_{ts}_bendindex_{bending_idx}_{seeds[i]}.pdf\")\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image_from_seed(bend_generator_clip, seed=seeds[i]) for i in range(16)]\n",
    "\n",
    "ts = tgt_text.replace(\" \", \"_\")\n",
    "for i, img in enumerate(exampleimgs):\n",
    "    img.save(f\"butterfly_clip_{ts}_bendindex_{bending_idx}_{seeds[i]}.pdf\")\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image_from_seed(bend_generator_clip, seed=seeds[i]) for i in range(16)]\n",
    "\n",
    "ts = tgt_text.replace(\" \", \"_\")\n",
    "for i, img in enumerate(exampleimgs):\n",
    "    img.save(f\"butterfly_clip_{ts}_bendindex_{bending_idx}_{seeds[i]}.pdf\")\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image_from_seed(bend_generator_clip, seed=seeds[i]) for i in range(16)]\n",
    "\n",
    "ts = tgt_text.replace(\" \", \"_\")\n",
    "for i, img in enumerate(exampleimgs):\n",
    "    img.save(f\"butterfly_clip_{ts}_bendindex_{bending_idx}_{seeds[i]}.pdf\")\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image_from_seed(bend_generator_clip, seed=seeds[i]) for i in range(16)]\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bendingmod_clip, bend_generator_clip, text_prompt, nce_loss, noise_input#, b_in, b_out\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional with coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new bending module to optimize\n",
    "# with CLIP loss\n",
    "\n",
    "device = 'cuda'\n",
    "numchans = [1024, 1024, 512, 256, 128, 64, 6]\n",
    "inputsizes = [8, 16, 32, 64, 128, 256, 512]\n",
    "\n",
    "bending_idx = 5\n",
    "\n",
    "bendingmod_clip = BendingConvModule_XY(numchans[bending_idx],\n",
    "                                       inputsizes[bending_idx])\n",
    "\n",
    "bend_generator_clip = BendedGenerator.from_pretrained(\"ceyda/butterfly_cropped_uniq1K_512\",\n",
    "                                                 bending_module=bendingmod_clip,\n",
    "                                                 bending_idx=bending_idx,\n",
    "                                                 train_bending=True)\n",
    "bend_generator_clip = bend_generator_clip.to(device)\n",
    "\n",
    "tgt_text = 'A gang of biker pumpkins painted by Jan van Eyck'\n",
    "tgt_text = 'A tree painted by Cezanne'\n",
    "#text_prompt = TextPrompt(tgt_text, device=device)\n",
    "nce_loss = NCELoss(tgt_text, temperature=0.01, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "n_iter = 2000\n",
    "\n",
    "opt = Adam(bendingmod_clip.parameters(), 1e-3)\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "for i in tqdm(range(n_iter)):\n",
    "    \n",
    "    noise_input = torch.randn(batch_size, \n",
    "                    bend_generator_clip.latent_dim, \n",
    "                    device=device)\n",
    "    \n",
    "    out = bend_generator_clip(noise_input)\n",
    "    out = out.clamp_(0., 1.)\n",
    "        \n",
    "\n",
    "    loss = nce_loss(out)\n",
    "    loss_log.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    \n",
    "plt.plot(range(n_iter), loss_log)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image_from_seed(bend_generator_clip, seed=seeds[i]) for i in range(16)]\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bendingmod_clip, bend_generator_clip, nce_loss, noise_input#, b_in, b_out\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new bending module to optimize\n",
    "# with CLIP loss\n",
    "\n",
    "numchans = [1024, 1024, 512, 256, 128, 64, 6]\n",
    "inputsizes = [8, 16, 32, 64, 128, 256, 512]\n",
    "\n",
    "bending_idx = 3\n",
    "\n",
    "bendingmod_clip = BendingConvModule(numchans[bending_idx],\n",
    "                                    act_fn='relu')\n",
    "bendsorting_clip = BendingDiffSort_XY(numchans[bending_idx],\n",
    "                                   inputsizes[bending_idx],\n",
    "                                   perm_rows=True, perm_cols=False)\n",
    "combined_bendmodule = ConcatenatedModules([bendsorting_clip, bendingmod_clip])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bend_generator_sort = BendedGenerator.from_pretrained(\"ceyda/butterfly_cropped_uniq1K_512\",\n",
    "                                                 bending_module=combined_bendmodule,\n",
    "                                                 bending_idx=bending_idx,\n",
    "                                                 train_bending=True)\n",
    "bend_generator_sort = bend_generator_sort.to(device)\n",
    "\n",
    "tgt_text = 'A tree painted by Cezanne'\n",
    "tgt_text = 'Low-poly rendering of Benjamin Franklin going to Venice'\n",
    "tgt_text = 'Low-poly rendering of Benjamin Franklin going to Venice'\n",
    "text_prompt = TextPrompt(tgt_text, device=device)\n",
    "nce_loss = NCELoss(tgt_text, device=device, temperature=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "import random\n",
    "random.seed(23456)\n",
    "np.random.seed(54321)\n",
    "torch.manual_seed(12345)\n",
    "#torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "n_iter = 1000\n",
    "\n",
    "div_loss = False\n",
    "div_weight = 6.\n",
    "div_loss_clip = False\n",
    "div_clip_weight = 6.\n",
    "\n",
    "opt = Adam(combined_bendmodule.parameters(), 1e-4)\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "for i in tqdm(range(n_iter)):\n",
    "    \n",
    "    noise_input = torch.randn(batch_size, \n",
    "                    bend_generator_sort.latent_dim, \n",
    "                    device=device)\n",
    "    \n",
    "    out, b_in, _ = bend_generator_sort(noise_input, return_inout=True)\n",
    "    out = out.clamp_(0., 1.)\n",
    "        \n",
    "    if div_loss_clip:\n",
    "        loss, clip_div = text_prompt(out, diversity=True)\n",
    "        loss += div_clip_weight * clip_div\n",
    "    else:\n",
    "        loss = nce_loss(out) #text_prompt(out)\n",
    "    if div_loss:\n",
    "        loss += div_weight * compute_diversity_loss(out, b_in)\n",
    "    \n",
    "    loss_log.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    \n",
    "plt.plot(range(n_iter), loss_log)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image_from_seed(bend_generator_sort, seed=seeds[i]) for i in range(16)]\n",
    "image_grid(exampleimgs, 4, 4)\n",
    "ts = tgt_text.replace(\" \", \"_\")\n",
    "for i, img in enumerate(exampleimgs):\n",
    "    img.save(f\"butterfly_diffsort_clip_{ts}_bendindex_{bending_idx}_{seeds[i]}.pdf\")\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image_from_seed(bend_generator_sort, seed=seeds[i]) for i in range(16)]\n",
    "image_grid(exampleimgs, 4, 4)\n",
    "for i, img in enumerate(exampleimgs):\n",
    "    img.save(f\"butterfly_diffsort_clip_{ts}_bendindex_{bending_idx}_{seeds[i]}.pdf\")\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image_from_seed(bend_generator_sort, seed=seeds[i]) for i in range(16)]\n",
    "image_grid(exampleimgs, 4, 4)\n",
    "for i, img in enumerate(exampleimgs):\n",
    "    img.save(f\"butterfly_diffsort_clip_{ts}_bendindex_{bending_idx}_{seeds[i]}.pdf\")\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image_from_seed(bend_generator_sort, seed=seeds[i]) for i in range(16)]\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image_from_seed(bend_generator_sort, seed=seeds[i]) for i in range(16)]\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image_from_seed(bend_generator_sort, seed=seeds[i]) for i in range(16)]\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del combined_bendmodule, bend_generator_sort, text_prompt, nce_loss, noise_input#, b_in, b_out\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image(bend_generator_sort) for _ in range(16)]\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image(bend_generator_sort) for _ in range(16)]\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image(bend_generator_sort) for _ in range(16)]\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image(bend_generator_sort) for _ in range(16)]\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image(bend_generator_sort) for _ in range(16)]\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image(bend_generator_sort) for _ in range(16)]\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control: does diff. sorting do better than random permutation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new bending module to optimize\n",
    "# with CLIP loss\n",
    "\n",
    "numchans = [1024, 1024, 512, 256, 128, 64, 6]\n",
    "inputsizes = [8, 16, 32, 64, 128, 256, 512]\n",
    "\n",
    "bending_idx = 1\n",
    "\n",
    "bendingmod_clip = BendingConvModule(numchans[bending_idx],\n",
    "                                    act_fn='relu')\n",
    "perm_h = torch.randperm(inputsizes[bending_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bend_generator_sort = BendedGenerator.from_pretrained(\"ceyda/butterfly_cropped_uniq1K_512\",\n",
    "                                                 bending_module=bendingmod_clip,\n",
    "                                                 bending_idx=bending_idx,\n",
    "                                                 train_bending=True)\n",
    "bend_generator_sort = bend_generator_sort.to(device)\n",
    "\n",
    "tgt_text = 'Peaches in a greek temple, 8-bit art'\n",
    "text_prompt = TextPrompt(tgt_text, device=device)\n",
    "nce_loss = NCELoss(tgt_text, device=device, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "n_iter = 1000\n",
    "\n",
    "div_loss = False\n",
    "div_weight = 6.\n",
    "div_loss_clip = False\n",
    "div_clip_weight = 6.\n",
    "\n",
    "opt = Adam(bendingmod_clip.parameters(), 1e-4)\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "for i in tqdm(range(n_iter)):\n",
    "    \n",
    "    noise_input = torch.randn(batch_size, \n",
    "                    bend_generator_sort.latent_dim, \n",
    "                    device=device)\n",
    "    \n",
    "    out = bend_generator_sort(noise_input, perm_h=perm_h)\n",
    "    out = out.clamp_(0., 1.)\n",
    "        \n",
    "    if div_loss_clip:\n",
    "        loss, clip_div = text_prompt(out, diversity=True)\n",
    "        loss += div_clip_weight * clip_div\n",
    "    else:\n",
    "        loss = nce_loss(out) #text_prompt(out)\n",
    "    \n",
    "    loss_log.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    \n",
    "plt.plot(range(n_iter), loss_log)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleimgs = [generate_image(bend_generator_sort) for _ in range(16)]\n",
    "image_grid(exampleimgs, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = torch.meshgrid(torch.arange(32),\n",
    "                      torch.arange(32),\n",
    "                      indexing='xy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinx = torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(sinx.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siny = torch.cos(y)\n",
    "plt.matshow(siny.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "3f88c79eab5e25d07d6227213314c9478e4141cdc655f57022914668adb63db5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
