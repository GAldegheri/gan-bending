{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giaco\\anaconda3\\envs\\giatorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from infonce import InfoNCE\n",
    "from clip import CLIP\n",
    "import open_clip\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_crossval(x, batch_size):\n",
    "    \"\"\"\n",
    "    Simple utility to remove first element from the first\n",
    "    (repeated) batch, second from second batch etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_size = x.shape[0]\n",
    "    removerows = torch.arange(batch_size) * (batch_size + 1)\n",
    "    keeprows = torch.LongTensor([i for i in \\\n",
    "        torch.arange(total_size) if i not in removerows])\n",
    "    \n",
    "    return x[keeprows, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 6\n",
    "x = torch.arange(bs).reshape(bs, 1).repeat(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4],\n",
       "        [5, 5, 5, 5, 5]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_keys = filter_crossval(x.repeat(bs, 1), bs).reshape(\n",
    "    bs, bs-1, -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 5, 5])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_keys[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_keys[1, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2966, -0.7131, -1.0321, -1.4339],\n",
       "        [ 0.8856,  1.0936, -0.3430, -1.8222],\n",
       "        [-0.2206,  0.1024, -0.4648, -1.7506]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2966, -0.7131, -1.0321, -1.4339],\n",
       "        [ 0.8856,  1.0936, -0.3430, -1.8222],\n",
       "        [-0.2206,  0.1024, -0.4648, -1.7506],\n",
       "        [-1.2966, -0.7131, -1.0321, -1.4339],\n",
       "        [ 0.8856,  1.0936, -0.3430, -1.8222],\n",
       "        [-0.2206,  0.1024, -0.4648, -1.7506]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.repeat(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2966, -0.7131, -1.0321, -1.4339],\n",
       "        [-1.2966, -0.7131, -1.0321, -1.4339],\n",
       "        [ 0.8856,  1.0936, -0.3430, -1.8222],\n",
       "        [ 0.8856,  1.0936, -0.3430, -1.8222],\n",
       "        [-0.2206,  0.1024, -0.4648, -1.7506],\n",
       "        [-0.2206,  0.1024, -0.4648, -1.7506]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.repeat_interleave(2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "X_len = 64\n",
    "removerows = torch.arange(batch_size)*(1 + batch_size)\n",
    "keeprows = torch.LongTensor([i for i in torch.arange(X_len) if i not in removerows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  9, 18, 27, 36, 45, 54, 63])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removerows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20,\n",
       "        21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40,\n",
       "        41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60,\n",
       "        61, 62])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keeprows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2453,  1.6633, -0.7840,  0.4265])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m X \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m64\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m X[keeprows, :]\n",
      "\u001b[1;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "X = torch.randn(64, 4)\n",
    "X[keeprows, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.randn(3, 3)\n",
    "x2 = torch.randn(3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.matmul(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = InfoNCE(reduction='none')\n",
    "batch_size, num_negative, embedding_size = 32, 48, 128\n",
    "query = torch.randn(batch_size, embedding_size)\n",
    "positive_key = torch.randn(batch_size, embedding_size)\n",
    "negative_keys = torch.randn(num_negative, embedding_size)\n",
    "output = loss(query, positive_key, negative_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Generic CLIP to inherit from\n",
    "    \"\"\"\n",
    "    def __init__(self, device='cpu'):\n",
    "        super(CLIP, self).__init__()\n",
    "        \n",
    "        self.clip_model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        'ViT-B-32-quickgelu', pretrained='laion400m_e32'\n",
    "        )\n",
    "        self.clip_model.to(device)\n",
    "        \n",
    "        # Create transforms to feed images to CLIP:\n",
    "        self.clip_tfms = T.Compose(preprocess.transforms[:2]+preprocess.transforms[-1:])\n",
    "        \n",
    "        # As a bonus, we can do some augmentation\n",
    "        self.aug_tfms = T.Compose([\n",
    "            T.RandomResizedCrop(480),\n",
    "            T.RandomAffine(5),\n",
    "            T.ColorJitter(),\n",
    "            T.GaussianBlur(5)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = CLIP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPrompt(CLIP):\n",
    "    def __init__(self, prompt_text, device='cpu'):\n",
    "        super(TextPrompt, self).__init__(device=device)\n",
    "        \n",
    "        self.prompt_text = prompt_text\n",
    "        with torch.no_grad():\n",
    "            tokenized_text = open_clip.tokenize([prompt_text]).to(device)\n",
    "            self.prompt_embed = self.clip_model.encode_text(tokenized_text)\n",
    "        \n",
    "    def forward(self, x, augment=True, return_mean=True,\n",
    "                diversity=False):\n",
    "        \"\"\"\n",
    "        Take a batch of images (x), encode them with clip_model\n",
    "        and score each with the prompt using Squared Great Circle Distance\n",
    "        (Lower is better).\n",
    "        \"\"\"\n",
    "        if augment:\n",
    "            x = self.aug_tfms(x)\n",
    "        image_embeds = self.clip_model.encode_image(self.clip_tfms(x))\n",
    "        input_normed = F.normalize(image_embeds.unsqueeze(1), dim=2)\n",
    "        embed_normed = F.normalize(self.prompt_embed.unsqueeze(0), dim=2)\n",
    "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
    "        \n",
    "        if diversity:\n",
    "            batch_size = x.shape[0]\n",
    "            assert batch_size % 2 == 0\n",
    "            img_embeds1 = input_normed[np.arange(0, batch_size, 2), ...]\n",
    "            img_embeds2 = input_normed[np.arange(1, batch_size, 2), ...]\n",
    "            x1 = x[np.arange(0, batch_size, 2), \n",
    "                   ...].reshape(batch_size//2, -1)\n",
    "            x2 = x[np.arange(1, batch_size, 2), \n",
    "                    ...].reshape(batch_size//2, -1)\n",
    "            div_latents = img_embeds1.sub(img_embeds2).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
    "            div_inputs = F.normalize(torch.mean(torch.abs(x1 - x2), axis=1), dim=0).reshape(-1, 1)\n",
    "            \n",
    "            diversities = div_latents#/div_inputs\n",
    "            if return_mean:\n",
    "                return dists.mean(), diversities.mean()\n",
    "            return dists, diversities\n",
    "        \n",
    "        if return_mean:\n",
    "            return dists.mean()\n",
    "        return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = TextPrompt('A laughing pumpkin', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(20, 3, 512, 512).cuda()\n",
    "y = prompt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1126, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giatorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f88c79eab5e25d07d6227213314c9478e4141cdc655f57022914668adb63db5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
